---
title: "Collaborating with Git"
author: "Jock Currie, Natasha Besseling"
date: last-modified
date-format: "D MMMM YYYY"
format: 
  html:
    toc: true
    toc-depth: 2
    number-sections: true
    fig-cap-location: bottom
    fig-numbering: true
bibliography: link2zotero.bib
execute:
  echo: true
  warning: false
  error: false
---

# Getting organised

Your greatest collaborator ever is likely to be your future self. Do yourself (and all other collaborators) a great service by keeping an organised and well documented project that they can navigate and understand with relative ease. The biggest impediment to collaboration is finding (and understanding) the pertinent information.

There are different ways of structuring your projects to keep them organised. In our team we try to keep a fairly standardised structure as below (recognising that some projects will require unique structures):

-   ***project*** (root) folder
    -   The "home" of your stand-alone project. It should typically contain everything the project needs and all the resultant outputs, organised in meaningful folders. It also contains your *README.md*, *.Rproj* and *.gitignore* files (which are created automatically).\
-   ***data*** folder
    -   all your input data (often included via a **link**); these files generally do not get altered or over-written by your project scripts.\
-   ***scripts*** folder
    -   all your scripts that contribute to your workflow (the 'recipes' of your work)\
-   ***outputs*** folder
    -   any and all results saved by the scripts (besides the images that we prefer to put in plots); these could be further divided into types of outputs if you would like, although we generally don't bother.\
-   ***plots*** folder
    -   any outputs that consist of image files (plots/figures) that you may want to include in reports or simply visualise your results or maps with after running the script(s)

Therefore, anything written (saved) by the scripts is written to "outputs/..." or to "plots/...", not into the root directory as that might get crowded and chaotic otherwise. We also want to (generally) avoid writing any changes to the input files, so we tend to not write to the *data* directory. See our [project_template](https://gitlab.com/jockongl/r_project_template) if interested.

## How do we avoid many duplicated input files across different projects? {#sec-avoidDupl}

One solution is the use of symbolic links (or directory junctions), which you create inside your project *data* directory and which point to a folder that is stored elsewhere on your computer and contains the input data relevant to the project.

In the Windows Command Prompt (cmd), use the [mklink](https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/mklink) function in the following way:

mklink /j “absolute_path\\to\\your\\Rproject\\data\\link2gis” “absolute_path\\to\\your\\data\\gis”

to create a linked folder (first set of inverted commas) that points to your target folder (second set of inveted commas). 
Hint: copy the absolute paths from your Windows Explorer.
If you repeat this approach across all your R projects, they can all point to a single directory that houses your input data somewhere else on your computer.

**NB Warning:** Make sure that you add the symbolic link, e.g. *data/link2gis* in the above example (or your entire *data/* folder) to the *.gitignore* file! Otherwise (\*on Windows), Git will try to stage and commit your entire linked data directory, which might not be intended and may often be huge.

By creating the above linked folders, together with the use of the R command *list.files()* to search for and load your input files, we can have complete interoperability between different machines (or collaborators), even if they have their data stored in different places (under different folder names).

```{r load_data, echo=TRUE}
### example

## load the spatial features and dplyr packages (install them if you have not yet done so)
library(sf)
library(dplyr)

## create file paths (find the correct path, which can vary among different machines)
mem_fl <- list.files(path = "../data", pattern = "^Marine_Ecosystem_Map_2023_final_pelagic_only.gpkg$", recursive = TRUE, full.names = TRUE) # we include '../' to go up one directory level to the root folder, because I created a 'quarto' folder and unlike other (normal .R) scripts, quarto files get rendered from the folder they are saved in. But, running it in the R console, we need to remove the '../' as the default working directory is the project root directory. this difference between the 'working directory' of quarto files and the R console or R scripts, can be avoided by using the here() function:
# mem_fl <- list.files(path = here::here("data"), pattern = "^Marine_Ecosystem_Map_2023_final_pelagic_only.gpkg$", recursive = TRUE, full.names = TRUE)

## load the spatial file 
mem <- read_sf(dsn=mem_fl)

## plot the pelagic ecosystem types
# plot(st_geometry(mem), axes=T)
mem %>% 
  st_transform(crs = 4326) %>% 
  select(P_EcosysType) %>% 
  plot(axes = T, key.pos = NULL, main = "Pelagic Ecosystem Types")

```

The file will be found as long as it exists somewhere within the path specified. To demonstrate, rename the 'pretend_link2gis' folder name and rerun it. Obviously, for consistent results and reproducibility, the collaborators need to ensure they have precisely the same input data! See @sec-datarepo for a trick to accomplish that.

# Version control

Once we have our data and file structures organised, we need to try and write organised and well-documented code. To track updates to the code, snapshot specific versions (e.g. of results shared with a collaborator or used in an output), and remain organised among multiple collaborators working on the same project, we need a version control system.

## What is Git?

A solution to **track** and **document** changes to files (inputs, outputs, results, document versions, etc). It allows you to take a snapshot of your files at a point in time, together with a commit message, so that you can roll back to that previous state (or recover individual files from that state) at any future time.

The (hidden) .git folder appears inside your project project folder once you have initialised Git on that project and it contains the (magic) history that tracks your committed changes over time.

## What is GitLab/GitHub?

An online platform that allows you to share your project with the public, or selected collaborators, and your collaborators are able to make changes that you can then 'accept' (merge) into your project. The updated versions or changes to the project are 'pushed' (from computer to online repository) and 'pulled' (from online repository to computer) with Git commands.

If you would like to version control your work on your own computer (and nothing further), then you need only Git (software) installed. If you would like to create an online repository (copy of your project) so that others can see it, or to be able to share your work with collaborators, then you will additionally need to create a profile on GitLab or GitHub (or equivalent) and 'push' your project to an online repository.

Git will track changes in any file formats, including Word documents, spatial files, etc. However, the *git diff* function will tend to show differences in the file **content** only for text files (which includes most script files).

# Common Git functions

Use cheat sheets when you're learning a new tool, e.g. [Git cheat sheet](https://education.github.com/git-cheat-sheet-education.pdf).

RStudio has integrated Git functionality and a Git interface on RStudio becomes visible once you have initialised Git version control on that project (close and reopen RStudio if it doesn't appear). The Git interface on RStudio provides buttons for the most common Git operations, which can alternatively be executed in a terminal. Sometimes you need to execute a command that is not covered by the buttons, so it is a good idea to become familiar with using the terminal (and many advanced users revert to using the terminal only).

Before you start using Git for the first time, you need to set your credentials within a terminal:\
git config --global user.name \[YOUR NAME\]\
git config --global user.email \[YOUR EMAIL\]

Thereafter, you can initialise (activate) git on any project with:\
**git init** (make sure you're in the project folder)\
In RStudio, the same can be achieved via *Tools* \> *Project Options* \> *Git/SVN*; select Git under *Version control system*.

**git commit**

**git diff**

**git clone \[url\]**

**git pull**

**git push**

The most frequent sequence used most of the time is:

*git pull* to ensure you have the newest version, after which you make your changes; *git add* to stage the changed files; *git commit* to commit the changes to a version and add a meaningful commit message; *git push* to push them to the remote repository;

## The importance of *.gitignore*

The *.gitignore* file is a text file that lists which files (or directories) should not be tracked by Git. In other words, those files/directories listed will not appear at all in the Git repository and any changes to them will not be tracked.

This is useful for many different contexts, e.g.:

-   sensitive input or output files you do not want shared with those accessing your project;\
-   your secret credentials saved in the *.Renviron* file (see @sec-renviron);\
-   large files that will cause you to exceed file size or repository size limits;
-   temporary files or other files that change frequently but have no bearing on project contents;

## The *.Renviron* file {#sec-renviron}

Any login credentials (e.g. username and password to access your databases) or other sensitive information that you want accessible to your R session, but which you do not want to share with collaborators (or the public if your project is public), you should add to a special file named *.Renviron*. This text file is treated in a special way, in that objects defined in it are automatically loaded into your R environment when you open your project in RStudio. If you want the same credentials accessible across all your R projects, you can save such information in an *.Renviron* file in your *HOME* directory. If you want different credentials accessible to different projects, you can instead create a project-specific *.Renviron* file in your project root directory. Remember, if you make changes in your .Renviron file, you need to restart R to load those changes.

**Very important:** Make a habit of adding '.Renviron' to your *.gitignore* file, so that the sensitive information you have put there is not pushed to your online repository!

## Things to watch out for

Always review and be certain about what is included or not included in *.gitignore*. We need to be careful to not share (push) sensitive data or credentials - e.g. it would be a horrible mistake to push your collaborator's highly sensitive data to a public Git repository!

Large file size limits and overall repository size limits need to be taken into consideration. For example, if you create (or include) a file \> 100 MB in your project, you'll be able to commit it on Git on your local machine, but when you try to push that commit to GitLab, it will be rejected (and can get complicated quickly).

# The importance of branches

Once a project has become mature (contains a substantial amount of work and is worth protecting), and especially if multiple people are collaborating on the same project, it becomes vital that further development moves to 'feature' branches and collaborators avoid making changes directly to the main branch. A certain feature is added (or improved) on the feature branch and only once it has been tested and reviewed (make sure it doesn't break anything!), does it get merged into the main branch. All development, exploration and testing of new methods or features happens on the feature branches and the main branch is 'protected'. Once their improvement/feature is complete and working as they intended, that collaborator opens a 'merge request'. Usually the owner(s) of the project are responsible for reviewing the changes and deciding whether they approve the merge or not.

Once you start collaborating on projects, it becomes especially important to pull and push frequently. Before you start working on a new feature, pull the remote project, in case your teammates have made some changes since you last worked on it. Otherwise, you'll be adding your changes to an outdated version!

# A centralised data repository (within SANBI IT systems) {#sec-datarepo}

Reproducible science requires us to apply exactly the same methods to exactly the same datasets. Sharing scripted workflows goes a long way to make the methods reproducible, but the other critical aspect is ensuring that the input data are identical. If we all have our own copy of input datasets on our machines, how do we ensure that they always remain the same, including when the data manager/owner adds new data or makes corrections? Below is a suggested approach that we are starting to implement. If anyone foresees problems with it, or if you have other solutions, please share!

Within the *SANBI Marine Programme* Team (Microsoft Teams), we have a 'marine_data' channel, where we are curating all of our spatial layers and datasets. Because it includes datasets that may not be shared with others and/or have usage restrictions, it is a private (protected) channel and membership is restricted to those who need it. Although not critical to this demonstration, the data are grouped in thematic subfolders and a data inventory (spreadsheet) in the root folder captures the metadata and the location (folder path) of the datasets. This Teams channel is intended to be our centralised and authoritative data storage. The advantage of using a Teams channel over a staff member's OneDrive account, is that the channel should be safe and persist irrespective of staff turnover. If datasets are updated or fixed, they should be updated here and tracked in the data inventory.

Up until a few days ago, we had to manually download these data and/or remember to re-download the relevant folder if data were added/changed. However, Natasha recently figured out a system by which we can sync the Teams channel to our computers. This is achieved by adding a shortcut to the Teams channel to our OneDrive account, and the OneDrive account is synced to our computers.

Lets try this:

1.  Navigate to your MS Teams and to the nba_data channel within the NBA_data team.

2.  Click on 'Add shortcut to OneDrive' in the options near the top (center) of the screen.

3.  If it is not already, set up your computer to [sync your OneDrive account](https://support.microsoft.com/en-us/office/sync-files-with-onedrive-in-windows-615391c4-2bd3-4aae-a42a-858262e42a49) (or certain parts of it), including the newly added *nba_data* folder.

If you manage that, you have a synced version of the Teams data folder on your computers and multiple people can access the same dataset. 
Any updates or changes made to that dataset on Teams will sync through to everyone's OneDrive and their computer (when they access that file next).

Lets try the same exercise as above in @sec-avoidDupl, but this time create a symbolic link from your project data drive to the nba_data folder we just added to OneDrive.
For me, this was typing the following into Windows Command Prompt:

mklink /j "C:\Users\J.Currie\Documents\Rprojects\git_tutorial\data\link2nba_data" "C:\Users\J.Currie\OneDrive\OneDrive - sanbi.org.za\shortcut_to_nba_data"

So, now we can all access the same nba_data datasets with our R projects. 

**But, beware:** 

1.  Never write to your data (input folder)! This rule becomes absolutely critical if we are all sharing access to a central authoritative data repository. They are 'our' data, not your personal copy! Do not make changes to them unless you are the authority or have explicit permission or instruction to do so. (Having read-only access would be ideal if we can configure that)

2.  Unless it is appropriate to push the data to the online repository with your project (and the data folder does not violate your online repo size limits), you may likely want to add the linked folder ('link2nba_data' in the above example) to your .gitignore file. I already did this for this tutorial, because I wanted you to add those data as a linked folder via your OneDrive account and not receive them when you cloned the online repo.

Lets load some of the data and play with them:

```{r load_nba_data, echo=TRUE}

## load the spatial features and dplyr packages (install them if you have not yet done so)
library(sf)
library(dplyr)

## create file paths (find the correct path, which can vary among different machines)
mem_fl <- list.files(path = here::here("data"), pattern = "^Marine_Ecosystem_Map_2023_final_pelagic_only.gpkg$", recursive = TRUE, full.names = TRUE)
threat_fl <- list.files(path = here::here("data"), pattern = "^nba_example_thr_data.csv$", recursive = TRUE, full.names = TRUE)


## load the spatial file 
mem <- read_sf(dsn=mem_fl)

## plot the pelagic ecosystem types
# plot(st_geometry(mem), axes=T)
mem %>% 
  st_transform(crs = 4326) %>% 
  select(P_EcosysType) %>% 
  plot(axes = T, key.pos = NULL, main = "Pelagic Ecosystem Types")

```

# General notes on good data hygiene (See Borer et al. 2009)

Use descriptive, standardised names for your files (and folders).

Use plain ASCII text for your file names, variable names, and data values. Avoid spaces, brackets or other special characters that will cause you pain in programming tasks.

Have your data in long table format (add data in rows, not columns).

Each column should contain only one type (class) of information (either text, numeric, etc.).

Always maintain effective metadata: clear, unambiguous, comprehensive

Use a scripted program for analysis. Store data in non-proprietary software formats. Consider hardware formats & longevity (save data from those floppy disks!). Always store an uncorrected, original data file.

# References

Borer ET, Seabloom EW, Jones MB, Schildhauer M (2009) Some Simple Guidelines for Effective Data Management. The Bulletin of the Ecological Society of America 90:205–214. doi: 10.1890/0012-9623-90.2.205
